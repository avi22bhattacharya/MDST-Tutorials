{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the same `states_edu.csv` that you should already be familiar with from the tutorial.\n",
    "\n",
    "We investigated Grade 8 reading score in the tutorial. For this checkpoint, you are asked to investigate another test. Here's an overview:\n",
    "\n",
    "* Choose a specific response variable to focus on\n",
    ">Grade 4 Math, Grade 4 Reading, Grade 8 Math\n",
    "* Pick or create features to use\n",
    ">Will all the features be useful in predicting test score? Are some more important than others? Should you standardize, bin, or scale the data?\n",
    "* Explore the data as it relates to that test\n",
    ">Create at least 2 visualizations (graphs), each with a caption describing the graph and what it tells us about the data\n",
    "* Create training and testing data\n",
    ">Do you want to train on all the data? Only data from the last 10 years? Only Michigan data?\n",
    "* Train a ML model to predict outcome \n",
    ">Define what you want to predict, and pick a model in sklearn to use (see sklearn <a href=\"https://scikit-learn.org/stable/modules/linear_model.html\">regressors</a>.\n",
    "* Summarize your findings\n",
    ">Write a 1 paragraph summary of what you did and make a recommendation about if and how student performance can be predicted\n",
    "\n",
    "Include comments throughout your code! Every cleanup and preprocessing task should be documented.\n",
    "\n",
    "Of course, if you're finding this assignment interesting (and we really hope you do!), you are welcome to do more than the requirements! For example, you may want to see if expenditure affects 4th graders more than 8th graders. Maybe you want to look into the extended version of this dataset and see how factors like sex and race are involved. You can include all your work in this notebook when you turn it in -- just always make sure you explain what you did and interpret your results. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: \n",
    "\n",
    "- You are being evaluated for compeletion and effort in this checkpoint. \n",
    "- Avoid manual labor / hard coding as much as possible, everything we've taught you so far are meant to simplify and automate your process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Cleanup </h2>\n",
    "\n",
    "Import `numpy`, `pandas`, and `matplotlib`.\n",
    "\n",
    "(Feel free to import other libraries!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the \"states_edu.csv\" dataset and take a look at the head of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp = pd.read_csv('../data/states_edu.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always familiarize yourself with what each column in the dataframe represents. Read about the states_edu dataset here: https://www.kaggle.com/noriuk/us-education-datasets-unification-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this space to rename columns, deal with missing data, etc. _(optional)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp.rename({\n",
    "    'GRADES_PK_G':'ENROLL_PREK',\n",
    "    'GRADES_KG_G':'ENROLL_KINDER',\n",
    "    'GRADES_4_G':'ENROLL_4',\n",
    "    'GRADES_8_G':'ENROLL_8',\n",
    "    'GRADES_12_G':'ENROLL_12',\n",
    "    'GRADES_1_8_G':'ENROLL_PRIMARY',\n",
    "    'GRADES_9_12_G':'ENROLL_HS',\n",
    "    'GRADES_ALL_G':'ENROLL_ALL',\n",
    "    'ENROLL':'ENROLL_ALL_EST'\n",
    "    },\n",
    "    axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1012.000000\n",
       "mean        0.523796\n",
       "std         1.225569\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.297826\n",
       "max         7.892933\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftemp[\"ENROLL_ALL\"].isna().sum()\n",
    "(dftemp[\"ENROLL_ALL\"]-dftemp[\"ENROLL_PREK\"]-dftemp[\"ENROLL_KINDER\"]-dftemp[\"ENROLL_PRIMARY\"]-dftemp[\"ENROLL_HS\"]).describe()\n",
    "((dftemp[\"ENROLL_ALL\"]-dftemp[\"ENROLL_PREK\"]-dftemp[\"ENROLL_KINDER\"]-dftemp[\"ENROLL_PRIMARY\"]-dftemp[\"ENROLL_HS\"])/dftemp[\"ENROLL_ALL\"]*100).describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploratory Data Analysis (EDA) </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen Outcome Variable for Test: Grade 4 Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many years of data are logged in our dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(dftemp['YEAR'].max() - dftemp['YEAR'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Michigan to Ohio. Which state has the higher average outcome score across all years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp2 = dftemp.groupby('STATE')['AVG_READING_4_SCORE'].mean()\n",
    "dfmich = dftemp2.loc['MICHIGAN']\n",
    "dfoh = dftemp2.loc['OHIO']\n",
    "print(\"Michigan: \", dfmich)\n",
    "print(\"Ohio: \", dfoh)\n",
    "print(\"Ohio has the higher average outcome score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average for your outcome score across all states in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2019 = dftemp[(dftemp['YEAR'] == 2019)]\n",
    "df2019['AVG_READING_4_SCORE'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum outcome score for every state. \n",
    "\n",
    "Refer to the `Grouping and Aggregating` section in Tutorial 0 if you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp.groupby('STATE')['AVG_READING_4_SCORE'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Engineering </h2>\n",
    "\n",
    "After exploring the data, you can choose to modify features that you would use to predict the performance of the students on your chosen response variable. \n",
    "\n",
    "You can also create your own features. For example, perhaps you figured that maybe a state's expenditure per student may affect their overall academic performance so you create a expenditure_per_student feature.\n",
    "\n",
    "Use this space to modify or create features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp['instr'] = dftemp['INSTRUCTION_EXPENDITURE']/dftemp['TOTAL_EXPENDITURE']\n",
    "dftemp['cap'] = dftemp['CAPITAL_OUTLAY_EXPENDITURE']/dftemp['TOTAL_EXPENDITURE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering justification: I decided to see the ratio of total expenditure that goes into paying teachers/instructors and the capital outlay, i.e. the facilites and infrastructure available to the students. The first variable will tell us how well the teachers are being paid, since a better pay would lead to better quality of teaching from their part, as well as a higher probability of being better role models and iecouraging their students to take up reading/participate in the NAEP reading exam. The second variable will tell us about how much investment is going into giving students the facilities and opportunities they need to take the NAEP exams. For example, better libraries and testing conditions for the exam would all factor into the score of the students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization</h2>\n",
    "\n",
    "Investigate the relationship between your chosen response variable and at least two predictors using visualizations. Write down your observations.\n",
    "\n",
    "**Visualization 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp.plot.scatter(x=\"instr\", y='AVG_READING_4_SCORE')\n",
    "plt.ylabel('4th Grade Reading Score')\n",
    "plt.xlabel('Ratio of expenditure on instructors')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a linear relationship between the two variables, although it is not very strong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftemp.plot.scatter(x=\"cap\", y='AVG_READING_4_SCORE')\n",
    "plt.ylabel('4th Grade Reading Score')\n",
    "plt.xlabel('Ratio of expenditure on capital')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two variables seem to have a linear, negative correlational relationship. This might be due to the fact that most of the expenditure on capital may not go towards improving the reading skills of the students, but on other factors such as sports equipment, classroom equipment, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Creation </h2>\n",
    "\n",
    "_Use this space to create train/test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dftemp[['instr', 'cap']].dropna()\n",
    "y = dftemp.loc[X.index]['AVG_READING_4_SCORE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.fillna(y.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Models [Resource](https://medium.com/@vijaya.beeravalli/comparison-of-machine-learning-classification-models-for-credit-card-default-data-c3cf805c9a5a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose some metrics to evaluate the performance of your model, some of them are mentioned in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(X_test, y_test))\n",
    "print(np.mean(model.predict(X_test)-y_test))\n",
    "print(np.mean(np.abs(model.predict(X_test)-y_test)))\n",
    "print(np.mean((model.predict(X_test)-y_test)**2)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have copied over the graphs that visualize the model's performance on the training and testing set. \n",
    "\n",
    "Change `col_name` and modify the call to `plt.ylabel()` to isolate how a single predictor affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'instr'\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_train[col_name], y_train, color = \"red\")\n",
    "plt.scatter(X_train[col_name], model.predict(X_train), color = \"green\")\n",
    "\n",
    "plt.legend(['True Training','Predicted Training'])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('Grade 4 Reading Score')\n",
    "plt.title(\"Model Behavior On Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = 'cap'\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_test[col_name], y_test, color = \"blue\")\n",
    "plt.scatter(X_test[col_name], model.predict(X_test), color = \"black\")\n",
    "\n",
    "plt.legend(['True testing','Predicted testing'])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel('Grade 4 Reading Score')\n",
    "plt.title(\"Model Behavior on Testing Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Summary </h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows a positive relationship between Grade 4 reading score and the ratio of total expenditure on instructors. It also shows a negative relationship between Grade 4 reading score and the ratio of total expenditure used on capital. I think my model doesn't seem to do a very well job on the training set, which can be seen by the evaluation statistics calculated above. This could be due to poor choice of variables or maybe the way the data is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "63ea159a95aeafbbd27753fb0090776dd7c23226bc2dd86cd8ec2559c5006c6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
